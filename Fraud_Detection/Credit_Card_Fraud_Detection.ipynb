{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
      "0           0   2019-01-01 00:00:18  2703186189652095   \n",
      "1           1   2019-01-01 00:00:44      630423337322   \n",
      "2           2   2019-01-01 00:00:51    38859492057661   \n",
      "3           3   2019-01-01 00:01:16  3534093764340240   \n",
      "4           4   2019-01-01 00:03:06   375534208663984   \n",
      "\n",
      "                             merchant       category     amt      first  \\\n",
      "0          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer   \n",
      "1     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie   \n",
      "2                fraud_Lind-Buckridge  entertainment  220.11     Edward   \n",
      "3  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy   \n",
      "4                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   \n",
      "\n",
      "      last gender                        street  ...      lat      long  \\\n",
      "0    Banks      F                561 Perry Cove  ...  36.0788  -81.1781   \n",
      "1     Gill      F  43039 Riley Greens Suite 393  ...  48.8878 -118.2105   \n",
      "2  Sanchez      M      594 White Dale Suite 530  ...  42.1808 -112.2620   \n",
      "3    White      M   9443 Cynthia Court Apt. 038  ...  46.2306 -112.1138   \n",
      "4   Garcia      M              408 Bradley Rest  ...  38.4207  -79.4629   \n",
      "\n",
      "   city_pop                                job         dob  \\\n",
      "0      3495          Psychologist, counselling  1988-03-09   \n",
      "1       149  Special educational needs teacher  1978-06-21   \n",
      "2      4154        Nature conservation officer  1962-01-19   \n",
      "3      1939                    Patent attorney  1967-01-12   \n",
      "4        99     Dance movement psychotherapist  1986-03-28   \n",
      "\n",
      "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
      "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
      "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
      "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
      "3  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
      "4  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
      "\n",
      "   is_fraud  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Testing Data:\n",
      "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
      "0           0   2020-06-21 12:14:25  2291163933867244   \n",
      "1           1   2020-06-21 12:14:33  3573030041201292   \n",
      "2           2   2020-06-21 12:14:53  3598215285024754   \n",
      "3           3   2020-06-21 12:15:15  3591919803438423   \n",
      "4           4   2020-06-21 12:15:17  3526826139003047   \n",
      "\n",
      "                               merchant        category    amt   first  \\\n",
      "0                 fraud_Kirlin and Sons   personal_care   2.86    Jeff   \n",
      "1                  fraud_Sporer-Keebler   personal_care  29.84  Joanne   \n",
      "2  fraud_Swaniawski, Nitzsche and Welch  health_fitness  41.28  Ashley   \n",
      "3                     fraud_Haley Group        misc_pos  60.05   Brian   \n",
      "4                 fraud_Johnston-Casper          travel   3.19  Nathan   \n",
      "\n",
      "       last gender                       street  ...      lat      long  \\\n",
      "0   Elliott      M            351 Darlene Green  ...  33.9659  -80.9355   \n",
      "1  Williams      F             3638 Marsh Union  ...  40.3207 -110.4360   \n",
      "2     Lopez      F         9333 Valentine Point  ...  40.6729  -73.5365   \n",
      "3  Williams      M  32941 Krystal Mill Apt. 552  ...  28.5697  -80.8191   \n",
      "4    Massey      M     5783 Evan Roads Apt. 465  ...  44.2529  -85.0170   \n",
      "\n",
      "   city_pop                     job         dob  \\\n",
      "0    333497     Mechanical engineer  1968-03-19   \n",
      "1       302  Sales professional, IT  1990-01-17   \n",
      "2     34496       Librarian, public  1970-10-21   \n",
      "3     54767            Set designer  1987-07-25   \n",
      "4      1126      Furniture designer  1955-07-06   \n",
      "\n",
      "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
      "0  2da90c7d74bd46a0caf3777415b3ebd3  1371816865  33.986391  -81.200714   \n",
      "1  324cc204407e99f51b0d6ca0055005e7  1371816873  39.450498 -109.960431   \n",
      "2  c81755dbbbea9d5c77f094348a7579be  1371816893  40.495810  -74.196111   \n",
      "3  2159175b9efe66dc301f149d3d5abf8c  1371816915  28.812398  -80.883061   \n",
      "4  57ff021bd3f328f8738bb535c302a31b  1371816917  44.959148  -85.884734   \n",
      "\n",
      "   is_fraud  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv (r'C:\\CodSoft_Projects\\archive\\fraudTrain.csv')\n",
    "test_data = pd.read_csv(r'C:\\CodSoft_Projects\\archive\\fraudTest.csv')\n",
    "\n",
    "# Display the head of the training data\n",
    "print(\"Training Data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Display the head of the test data\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1296675 entries, 0 to 1296674\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count    Dtype  \n",
      "---  ------                 --------------    -----  \n",
      " 0   Unnamed: 0             1296675 non-null  int64  \n",
      " 1   trans_date_trans_time  1296675 non-null  object \n",
      " 2   cc_num                 1296675 non-null  int64  \n",
      " 3   merchant               1296675 non-null  object \n",
      " 4   category               1296675 non-null  object \n",
      " 5   amt                    1296675 non-null  float64\n",
      " 6   first                  1296675 non-null  object \n",
      " 7   last                   1296675 non-null  object \n",
      " 8   gender                 1296675 non-null  object \n",
      " 9   street                 1296675 non-null  object \n",
      " 10  city                   1296675 non-null  object \n",
      " 11  state                  1296675 non-null  object \n",
      " 12  zip                    1296675 non-null  int64  \n",
      " 13  lat                    1296675 non-null  float64\n",
      " 14  long                   1296675 non-null  float64\n",
      " 15  city_pop               1296675 non-null  int64  \n",
      " 16  job                    1296675 non-null  object \n",
      " 17  dob                    1296675 non-null  object \n",
      " 18  trans_num              1296675 non-null  object \n",
      " 19  unix_time              1296675 non-null  int64  \n",
      " 20  merch_lat              1296675 non-null  float64\n",
      " 21  merch_long             1296675 non-null  float64\n",
      " 22  is_fraud               1296675 non-null  int64  \n",
      "dtypes: float64(5), int64(6), object(12)\n",
      "memory usage: 227.5+ MB\n",
      "None\n",
      "\n",
      "Number of Duplicate Entries in Training Data: 0\n",
      "\n",
      "Training Data Description:\n",
      "         Unnamed: 0        cc_num           amt           zip           lat  \\\n",
      "count  1.296675e+06  1.296675e+06  1.296675e+06  1.296675e+06  1.296675e+06   \n",
      "mean   6.483370e+05  4.171920e+17  7.035104e+01  4.880067e+04  3.853762e+01   \n",
      "std    3.743180e+05  1.308806e+18  1.603160e+02  2.689322e+04  5.075808e+00   \n",
      "min    0.000000e+00  6.041621e+10  1.000000e+00  1.257000e+03  2.002710e+01   \n",
      "25%    3.241685e+05  1.800429e+14  9.650000e+00  2.623700e+04  3.462050e+01   \n",
      "50%    6.483370e+05  3.521417e+15  4.752000e+01  4.817400e+04  3.935430e+01   \n",
      "75%    9.725055e+05  4.642255e+15  8.314000e+01  7.204200e+04  4.194040e+01   \n",
      "max    1.296674e+06  4.992346e+18  2.894890e+04  9.978300e+04  6.669330e+01   \n",
      "\n",
      "               long      city_pop     unix_time     merch_lat    merch_long  \\\n",
      "count  1.296675e+06  1.296675e+06  1.296675e+06  1.296675e+06  1.296675e+06   \n",
      "mean  -9.022634e+01  8.882444e+04  1.349244e+09  3.853734e+01 -9.022646e+01   \n",
      "std    1.375908e+01  3.019564e+05  1.284128e+07  5.109788e+00  1.377109e+01   \n",
      "min   -1.656723e+02  2.300000e+01  1.325376e+09  1.902779e+01 -1.666712e+02   \n",
      "25%   -9.679800e+01  7.430000e+02  1.338751e+09  3.473357e+01 -9.689728e+01   \n",
      "50%   -8.747690e+01  2.456000e+03  1.349250e+09  3.936568e+01 -8.743839e+01   \n",
      "75%   -8.015800e+01  2.032800e+04  1.359385e+09  4.195716e+01 -8.023680e+01   \n",
      "max   -6.795030e+01  2.906700e+06  1.371817e+09  6.751027e+01 -6.695090e+01   \n",
      "\n",
      "           is_fraud  \n",
      "count  1.296675e+06  \n",
      "mean   5.788652e-03  \n",
      "std    7.586269e-02  \n",
      "min    0.000000e+00  \n",
      "25%    0.000000e+00  \n",
      "50%    0.000000e+00  \n",
      "75%    0.000000e+00  \n",
      "max    1.000000e+00  \n",
      "\n",
      "Categorical Data Description:\n",
      "       trans_date_trans_time           merchant       category        first  \\\n",
      "count                1296675            1296675        1296675      1296675   \n",
      "unique               1274791                693             14          352   \n",
      "top      2019-04-22 16:02:01  fraud_Kilback LLC  gas_transport  Christopher   \n",
      "freq                       4               4403         131659        26669   \n",
      "\n",
      "           last   gender                      street        city    state  \\\n",
      "count   1296675  1296675                     1296675     1296675  1296675   \n",
      "unique      481        2                         983         894       51   \n",
      "top       Smith        F  0069 Robin Brooks Apt. 695  Birmingham       TX   \n",
      "freq      28794   709863                        3123        5617    94876   \n",
      "\n",
      "                      job         dob                         trans_num  \n",
      "count             1296675     1296675                           1296675  \n",
      "unique                494         968                           1296675  \n",
      "top     Film/video editor  1977-03-23  0b242abb623afc578575680df30655b9  \n",
      "freq                 9779        5636                                 1  \n"
     ]
    }
   ],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"\\nTraining Data Info:\")\n",
    "print(train_data.info())\n",
    "\n",
    "# Check for duplicate entries\n",
    "print(\"\\nNumber of Duplicate Entries in Training Data:\", train_data.duplicated().sum())\n",
    "\n",
    "# Descriptive statistics for numerical features\n",
    "print(\"\\nTraining Data Description:\")\n",
    "print(train_data.describe())\n",
    "\n",
    "# Descriptive statistics for categorical features\n",
    "print(\"\\nCategorical Data Description:\")\n",
    "print(train_data.describe(include=['O']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Unnamed: 0'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\CodSoft_Projects\\Credit_Card_Fraud_Detection.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Dropping the 'Unnamed: 0' column since it is just index\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_data\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mUnnamed: 0\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test_data\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\100ab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[0;32m   5111\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   5112\u001b[0m     labels: IndexLabel \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   5120\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5121\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5122\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5257\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[0;32m   5259\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   5260\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5261\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[0;32m   5262\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   5263\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[0;32m   5264\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[0;32m   5265\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   5266\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\100ab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4551\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\100ab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\100ab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6697\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[0;32m   6698\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 6699\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(labels[mask])\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6700\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[0;32m   6701\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Unnamed: 0'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Dropping the 'Unnamed: 0' column since it is just index\n",
    "train_data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "test_data.drop('Unnamed: 0', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values in Training Data:\n",
      "trans_date_trans_time    0\n",
      "cc_num                   0\n",
      "merchant                 0\n",
      "category                 0\n",
      "amt                      0\n",
      "first                    0\n",
      "last                     0\n",
      "gender                   0\n",
      "street                   0\n",
      "city                     0\n",
      "state                    0\n",
      "zip                      0\n",
      "lat                      0\n",
      "long                     0\n",
      "city_pop                 0\n",
      "job                      0\n",
      "dob                      0\n",
      "trans_num                0\n",
      "unix_time                0\n",
      "merch_lat                0\n",
      "merch_long               0\n",
      "is_fraud                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(\"\\nMissing Values in Training Data:\")\n",
    "print(train_data.isnull().sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Handling outliers in the 'amt' column\n",
    "# This is a simplistic approach, and you should tailor it to your specific needs\n",
    "Q1 = train_data['amt'].quantile(0.25)\n",
    "Q3 = train_data['amt'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_cap = Q1 - 1.5 * IQR\n",
    "upper_cap = Q3 + 1.5 * IQR\n",
    "\n",
    "train_data['amt'] = train_data['amt'].clip(lower_cap, upper_cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Training Data:\n",
      "  trans_date_trans_time            cc_num                            merchant  \\\n",
      "0   2019-01-01 00:00:18  2703186189652095          fraud_Rippin, Kub and Mann   \n",
      "1   2019-01-01 00:00:44      630423337322     fraud_Heller, Gutmann and Zieme   \n",
      "2   2019-01-01 00:00:51    38859492057661                fraud_Lind-Buckridge   \n",
      "3   2019-01-01 00:01:16  3534093764340240  fraud_Kutch, Hermiston and Farrell   \n",
      "4   2019-01-01 00:03:06   375534208663984                 fraud_Keeling-Crist   \n",
      "\n",
      "        category      amt      first     last gender  \\\n",
      "0       misc_net    4.970   Jennifer    Banks      F   \n",
      "1    grocery_pos  107.230  Stephanie     Gill      F   \n",
      "2  entertainment  193.375     Edward  Sanchez      M   \n",
      "3  gas_transport   45.000     Jeremy    White      M   \n",
      "4       misc_pos   41.960      Tyler   Garcia      M   \n",
      "\n",
      "                         street            city  ...      lat      long  \\\n",
      "0                561 Perry Cove  Moravian Falls  ...  36.0788  -81.1781   \n",
      "1  43039 Riley Greens Suite 393          Orient  ...  48.8878 -118.2105   \n",
      "2      594 White Dale Suite 530      Malad City  ...  42.1808 -112.2620   \n",
      "3   9443 Cynthia Court Apt. 038         Boulder  ...  46.2306 -112.1138   \n",
      "4              408 Bradley Rest        Doe Hill  ...  38.4207  -79.4629   \n",
      "\n",
      "   city_pop                                job         dob  \\\n",
      "0      3495          Psychologist, counselling  1988-03-09   \n",
      "1       149  Special educational needs teacher  1978-06-21   \n",
      "2      4154        Nature conservation officer  1962-01-19   \n",
      "3      1939                    Patent attorney  1967-01-12   \n",
      "4        99     Dance movement psychotherapist  1986-03-28   \n",
      "\n",
      "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
      "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
      "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
      "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
      "3  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
      "4  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
      "\n",
      "   is_fraud  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# Final check\n",
    "print(\"\\nFinal Training Data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Saving the cleaned data\n",
    "train_data.to_csv('cleaned_fraud_train.csv', index=False)\n",
    "test_data.to_csv('cleaned_fraud_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the cleaned data\n",
    "train_data = pd.read_csv('cleaned_fraud_train.csv')\n",
    "test_data = pd.read_csv('cleaned_fraud_test.csv')\n",
    "\n",
    "# Extracting day of the week, hour from 'trans_date_trans_time'\n",
    "train_data['trans_date'] = pd.to_datetime(train_data['trans_date_trans_time'])\n",
    "test_data['trans_date'] = pd.to_datetime(test_data['trans_date_trans_time'])\n",
    "\n",
    "train_data['day_of_week'] = train_data['trans_date'].dt.dayofweek\n",
    "test_data['day_of_week'] = test_data['trans_date'].dt.dayofweek\n",
    "\n",
    "train_data['hour'] = train_data['trans_date'].dt.hour\n",
    "test_data['hour'] = test_data['trans_date'].dt.hour\n",
    "\n",
    "# Dropping the original 'trans_date_trans_time' column\n",
    "train_data.drop('trans_date_trans_time', axis=1, inplace=True)\n",
    "test_data.drop('trans_date_trans_time', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features\n",
    "features = ['amt', 'category', 'gender', 'city_pop', 'merchant', 'lat', 'long', 'job', 'day_of_week', 'hour', 'is_fraud']\n",
    "train_data = train_data[features]\n",
    "test_data = test_data[features]\n",
    "\n",
    "# Separating the target variable\n",
    "y_train = train_data['is_fraud']\n",
    "X_train = train_data.drop('is_fraud', axis=1)\n",
    "\n",
    "y_test = test_data['is_fraud']\n",
    "X_test = test_data.drop('is_fraud', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation: Normalization, standardization etc. especially because it is a distance based algorithm.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Identifying numerical and categorical columns\n",
    "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'bool']).columns\n",
    "\n",
    "# Creating transformers for numerical and categorical data\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Bundling transformers into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Applying the transformations\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Obtaining dependency information for imbalanced-learn from https://files.pythonhosted.org/packages/a3/9e/fbe60a768502af54563dcb59ca7856f5a8833b3ad5ada658922e1ab09b7f/imbalanced_learn-0.11.0-py3-none-any.whl.metadata\n",
      "  Downloading imbalanced_learn-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\100ab\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from imbalanced-learn->imblearn) (1.25.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\100ab\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from imbalanced-learn->imblearn) (1.11.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\100ab\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from imbalanced-learn->imblearn) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\100ab\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from imbalanced-learn->imblearn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\100ab\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from imbalanced-learn->imblearn) (3.2.0)\n",
      "Downloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n",
      "   ---------------------------------------- 0.0/235.6 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 30.7/235.6 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 112.6/235.6 kB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 194.6/235.6 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 235.6/235.6 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.11.0 imblearn-0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE, counts of label '1': 7506\n",
      "Before SMOTE, counts of label '0': 1289169 \n",
      "\n",
      "After SMOTE, counts of label '1': 1289169\n",
      "After SMOTE, counts of label '0': 1289169\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Checking class distribution\n",
    "print(\"Before SMOTE, counts of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"Before SMOTE, counts of label '0': {} \\n\".format(sum(y_train == 0)))\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Checking class distribution after applying SMOTE\n",
    "print(\"After SMOTE, counts of label '1': {}\".format(sum(y_train == 1)))\n",
    "print(\"After SMOTE, counts of label '0': {}\".format(sum(y_train == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis output indicates a significant class imbalance in the dataset: \\noriginally, there were 7,506 instances of the minority class (label '1', indicating fraud) \\ncompared to 1,289,169 instances of the majority class (label '0', indicating non-fraudulent transactions). \\nAfter applying SMOTE (Synthetic Minority Over-sampling Technique), \\nboth classes are balanced with 1,289,169 instances each.\\n\\nThis balancing is crucial for many machine learning models, \\nas they can be biased towards the majority class in imbalanced datasets, \\nleading to poor classification performance on the minority class. \\nBy using SMOTE, we synthetically generated new samples in the minority class, \\nwhich helps in improving the classifier's ability to detect fraudulent transactions.\\n\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This output indicates a significant class imbalance in the dataset: \n",
    "originally, there were 7,506 instances of the minority class (label '1', indicating fraud) \n",
    "compared to 1,289,169 instances of the majority class (label '0', indicating non-fraudulent transactions). \n",
    "After applying SMOTE (Synthetic Minority Over-sampling Technique), \n",
    "both classes are balanced with 1,289,169 instances each.\n",
    "\n",
    "This balancing is crucial for many machine learning models, \n",
    "as they can be biased towards the majority class in imbalanced datasets, \n",
    "leading to poor classification performance on the minority class. \n",
    "By using SMOTE, we synthetically generated new samples in the minority class, \n",
    "which helps in improving the classifier's ability to detect fraudulent transactions.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2062670 samples\n",
      "Validation set size: 515668 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Splitting the dataset into the Training set and Validation set\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train_final.shape[0]} samples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.81      0.81    257186\n",
      "           1       0.82      0.82      0.82    258482\n",
      "\n",
      "    accuracy                           0.82    515668\n",
      "   macro avg       0.82      0.82      0.82    515668\n",
      "weighted avg       0.82      0.82      0.82    515668\n",
      "\n",
      "Confusion Matrix:\n",
      " [[209515  47671]\n",
      " [ 47703 210779]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Building the Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Predicting the Validation set results\n",
    "y_pred_val_logreg = logreg.predict(X_val)\n",
    "\n",
    "# Evaluating the Logistic Regression model\n",
    "print(\"Logistic Regression - Validation Set:\")\n",
    "print(classification_report(y_val, y_pred_val_logreg))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred_val_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Validation Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    257186\n",
      "           1       1.00      1.00      1.00    258482\n",
      "\n",
      "    accuracy                           1.00    515668\n",
      "   macro avg       1.00      1.00      1.00    515668\n",
      "weighted avg       1.00      1.00      1.00    515668\n",
      "\n",
      "Confusion Matrix:\n",
      " [[256046   1140]\n",
      " [   588 257894]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Building the Decision Tree model\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "dtree.fit(X_train_final, y_train_final)\n",
    "\n",
    "# Predicting the Validation set results\n",
    "y_pred_val_dtree = dtree.predict(X_val)\n",
    "\n",
    "# Evaluating the Decision Tree model\n",
    "print(\"Decision Tree - Validation Set:\")\n",
    "print(classification_report(y_val, y_pred_val_dtree))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_pred_val_dtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(random_state=42)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choosing decision tree as the final model since it has a near perfect score on the validation set.\n",
    "\n",
    "dtree.fit(X_train, y_train)  # Re-train on the full training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Test Set Evaluation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    553574\n",
      "           1       0.25      0.34      0.29      2145\n",
      "\n",
      "    accuracy                           0.99    555719\n",
      "   macro avg       0.63      0.67      0.64    555719\n",
      "weighted avg       0.99      0.99      0.99    555719\n",
      "\n",
      "Confusion Matrix:\n",
      " [[551438   2136]\n",
      " [  1418    727]]\n",
      "ROC-AUC Score: 0.6674572870711057\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Your existing code for prediction and evaluation\n",
    "y_pred_test = dtree.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree - Test Set Evaluation:\")\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "# Compute ROC-AUC\n",
    "roc_auc = roc_auc_score(y_test, dtree.predict_proba(X_test)[:, 1])\n",
    "print(\"ROC-AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe model is almost perfect in identifying non-fraudulent transactions (Class 0) \\nwith precision and recall both close to 1.00.\\n\\nFor fraudulent transactions (Class 1), the precision is only 0.25, and recall is 0.34. \\nThis means the model correctly identifies only 34% of fraudulent transactions, \\nand when it predicts a transaction as fraudulent, it is correct only 25% of the time.\\n\\nThe model has a high overall accuracy of 0.99, \\nbut this metric is less informative due to the imbalanced nature of the dataset \\n(where non-fraudulent transactions vastly outnumber fraudulent ones).\\n\\nThe model has a high number of false negatives (fraudulent transactions missed) \\nand a moderate number of false positives \\n(non-fraudulent transactions incorrectly labeled as fraudulent).\\n\\nThe ROC-AUC score of 0.667 indicates moderate ability of the model \\nto distinguish between fraudulent and non-fraudulent transactions.\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The model is almost perfect in identifying non-fraudulent transactions (Class 0) \n",
    "with precision and recall both close to 1.00.\n",
    "\n",
    "For fraudulent transactions (Class 1), the precision is only 0.25, and recall is 0.34. \n",
    "This means the model correctly identifies only 34% of fraudulent transactions, \n",
    "and when it predicts a transaction as fraudulent, it is correct only 25% of the time.\n",
    "\n",
    "The model has a high overall accuracy of 0.99, \n",
    "but this metric is less informative due to the imbalanced nature of the dataset \n",
    "(where non-fraudulent transactions vastly outnumber fraudulent ones).\n",
    "\n",
    "The model has a high number of false negatives (fraudulent transactions missed) \n",
    "and a moderate number of false positives \n",
    "(non-fraudulent transactions incorrectly labeled as fraudulent).\n",
    "\n",
    "The ROC-AUC score of 0.667 indicates moderate ability of the model \n",
    "to distinguish between fraudulent and non-fraudulent transactions.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decision_tree_model.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since, uncertainty about the types of objects needing serialization so.pkl is used to handle the wide variety.\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Since 'dtree' is our trained Decision Tree model\n",
    "joblib.dump(dtree, 'decision_tree_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input():\n",
    "    # Asking user input for each feature\n",
    "    amt = float(input(\"Enter transaction amount: \"))\n",
    "    category = input(\"Enter transaction category (e.g., 'misc_net', 'grocery_pos', etc.): \")\n",
    "    gender = input(\"Enter gender (M/F): \")\n",
    "    city_pop = int(input(\"Enter city population: \"))\n",
    "    merchant = input(\"Enter merchant name: \")\n",
    "    lat = float(input(\"Enter latitude of the transaction location: \"))\n",
    "    long = float(input(\"Enter longitude of the transaction location: \"))\n",
    "    job = input(\"Enter job title: \")\n",
    "    dob = input(\"Enter date of birth (YYYY-MM-DD): \")\n",
    "    merch_lat = float(input(\"Enter merchant's latitude: \"))\n",
    "    merch_long = float(input(\"Enter merchant's longitude: \"))\n",
    "\n",
    "    # Handling date and time input\n",
    "    correct_format = False\n",
    "    while not correct_format:\n",
    "        trans_date_trans_time = input(\"Enter transaction date and time (YYYY-MM-DD HH:MM:SS): \")\n",
    "        try:\n",
    "            # Try to parse the date and time\n",
    "            parsed_date = pd.to_datetime(trans_date_trans_time)\n",
    "            correct_format = True\n",
    "        except ValueError:\n",
    "            # If parsing fails, inform the user and prompt again\n",
    "            print(\"Date and time format is incorrect. Please use YYYY-MM-DD HH:MM:SS format.\")\n",
    "\n",
    "    day_of_week = parsed_date.day_name()\n",
    "    hour = parsed_date.hour\n",
    "\n",
    "    user_input = {\n",
    "        'amt': amt,\n",
    "        'category': category,\n",
    "        'gender': gender,\n",
    "        'city_pop': city_pop,\n",
    "        'merchant': merchant,\n",
    "        'lat': lat,\n",
    "        'long': long,\n",
    "        'job': job,\n",
    "        'dob': dob,\n",
    "        'merch_lat': merch_lat,\n",
    "        'merch_long': merch_long,\n",
    "        'day_of_week': day_of_week,\n",
    "        'hour': hour\n",
    "    }\n",
    "\n",
    "    return user_input\n",
    "\n",
    "def load_model_and_predict(input_data):\n",
    "    # Load the saved model\n",
    "    model = joblib.load('decision_tree_model.pkl')\n",
    "\n",
    "    # Convert input_data to the format the model expects (e.g., DataFrame)\n",
    "    input_df = pd.DataFrame([input_data])\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(input_df)\n",
    "    return prediction[0]  # Returning the first (and only) prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\100ab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:457: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'grocery_pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\CodSoft_Projects\\Credit_Card_Fraud_Detection.ipynb Cell 21\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m user_input \u001b[39m=\u001b[39m get_user_input()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Make a prediction\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m prediction \u001b[39m=\u001b[39m load_model_and_predict(user_input)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m prediction \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe transaction is predicted to be fraudulent.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\CodSoft_Projects\\Credit_Card_Fraud_Detection.ipynb Cell 21\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#X30sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m input_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame([input_data])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#X30sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# Make a prediction\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#X30sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(input_df)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/CodSoft_Projects/Credit_Card_Fraud_Detection.ipynb#X30sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mreturn\u001b[39;00m prediction[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\100ab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\tree\\_classes.py:500\u001b[0m, in \u001b[0;36mBaseDecisionTree.predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Predict class or regression value for X.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \n\u001b[0;32m    479\u001b[0m \u001b[39mFor a classification model, the predicted class for each sample in X is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39m    The predicted classes, or the predict values.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    499\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 500\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X, check_input)\n\u001b[0;32m    501\u001b[0m proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree_\u001b[39m.\u001b[39mpredict(X)\n\u001b[0;32m    502\u001b[0m n_samples \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\100ab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\tree\\_classes.py:460\u001b[0m, in \u001b[0;36mBaseDecisionTree._validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     force_all_finite \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 460\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    461\u001b[0m     X,\n\u001b[0;32m    462\u001b[0m     dtype\u001b[39m=\u001b[39;49mDTYPE,\n\u001b[0;32m    463\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    464\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    465\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    466\u001b[0m )\n\u001b[0;32m    467\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (\n\u001b[0;32m    468\u001b[0m     X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc\n\u001b[0;32m    469\u001b[0m ):\n\u001b[0;32m    470\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\100ab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\100ab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[0;32m    918\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\100ab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39marray(array, order\u001b[39m=\u001b[39morder, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[39m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array)\n",
      "File \u001b[1;32mc:\\Users\\100ab\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:1998\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m   1997\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1998\u001b[0m     arr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(values, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   1999\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2000\u001b[0m         astype_is_view(values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   2001\u001b[0m         \u001b[39mand\u001b[39;00m using_copy_on_write()\n\u001b[0;32m   2002\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mgr\u001b[39m.\u001b[39mis_single_block\n\u001b[0;32m   2003\u001b[0m     ):\n\u001b[0;32m   2004\u001b[0m         \u001b[39m# Check if both conversions can be done without a copy\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m         \u001b[39mif\u001b[39;00m astype_is_view(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m], values\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m astype_is_view(\n\u001b[0;32m   2006\u001b[0m             values\u001b[39m.\u001b[39mdtype, arr\u001b[39m.\u001b[39mdtype\n\u001b[0;32m   2007\u001b[0m         ):\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'grocery_pos'"
     ]
    }
   ],
   "source": [
    "# Get user input\n",
    "user_input = get_user_input()\n",
    "\n",
    "# Make a prediction\n",
    "prediction = load_model_and_predict(user_input)\n",
    "if prediction == 1:\n",
    "    print(\"The transaction is predicted to be fraudulent.\")\n",
    "else:\n",
    "    print(\"The transaction is predicted to be legitimate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
